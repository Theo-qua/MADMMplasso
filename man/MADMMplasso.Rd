% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MADAMM11.R, R/MADMMplasso-package.R
\docType{package}
\name{MADMMplasso}
\alias{MADMMplasso}
\title{Fit a multi-response pliable lasso model over a path of regularization values}
\usage{
MADMMplasso(
  X,
  Z,
  y,
  alpha,
  my_lambda = NULL,
  lambda_min = 0.001,
  max_it = 50000,
  e.abs = 0.001,
  e.rel = 0.001,
  maxgrid,
  nlambda,
  rho = 5,
  my_print = FALSE,
  alph = 1.8,
  tree,
  cv = FALSE,
  parallel = TRUE,
  pal = 0,
  gg = NULL,
  tol = 1e-04,
  cl = 4
)
}
\arguments{
\item{X}{N by p matrix of predictors}

\item{Z}{N by K matrix of modifying variables. The elements of Z  may represent quantitative or categorical variables, or a mixture of the two.
Categorical varables should be coded by 0-1 dummy variables: for a k-level variable, one can use either k or k-1  dummy variables.}

\item{y}{N by D matrix  of responses. The X and Z variables are centered in the function. We recommmend that X and Z also be standardized before the call}

\item{alpha}{mixing parameter- default 0.5}

\item{my_lambda}{TODO: fill in}

\item{lambda_min}{TODO: fill in}

\item{max_it}{TODO: fill in}

\item{e.abs}{absolute error for the admm}

\item{e.rel}{relative error for the admm}

\item{maxgrid}{similar to nlambda}

\item{nlambda}{number of lambda values desired (default 50).}

\item{rho}{the Lagrange variable for the ADMM}

\item{my_print}{TODO: fill in}

\item{alph}{TODO: fill in}

\item{tree}{TODO: fill in}

\item{cv}{TODO: fill in}

\item{parallel}{TODO: fill in}

\item{pal}{TODO: fill in}

\item{gg}{penalty term for the tree structure}

\item{tol}{TODO: fill in}

\item{cl}{TODO: fill in}
}
\value{
predicted values for the MADMMplasso fit
}
\description{
TODO: add description (This function fits a multi-response pliable lasso model over a path of regularization values?)
}
\examples{
# Train the model
# generate some data
set.seed(1235)
N = 100 ; p =50;nz=4; K=nz
X <- matrix(rnorm(n = N * p), nrow = N, ncol = p)
mx=colMeans(X)
sx=sqrt(apply(X,2,var))
X=scale(X,mx,sx)
X=matrix(as.numeric(X),N,p)
Z =matrix(rnorm(N*nz),N,nz)
mz=colMeans(Z)
sz=sqrt(apply(Z,2,var))
Z=scale(Z,mz,sz)
beta_1 <- rep(x = 0, times = p)
beta_2<-rep(x = 0, times = p)
beta_3<-rep(x = 0, times = p)
beta_4<-rep(x = 0, times = p)
beta_5<-rep(x = 0, times = p)
beta_6<-rep(x = 0, times = p)


beta_1[1:5] <- c(2, 2, 2, 2,2)
beta_2[1:5]<-c(2, 2, 2, 2,2)
beta_3[6:10]<-c(2, 2, 2, -2,-2)
beta_4[6:10] <- c(2, 2, 2, -2,-2)
beta_5[11:15] <- c(-2,  -2,-2, -2,-2)
beta_6[11:15] <- c(-2, -2, -2, -2,-2)

Beta<-cbind(beta_1,beta_2,beta_3,beta_4,beta_5,beta_6)
colnames(Beta)<-c(1:6)


theta<-array(0,c(p,K,6))
theta[1,1,1]<-2;theta[3,2,1]<-2;theta[4,3,1]<- -2;theta[5,4,1]<- -2;
theta[1,1,2]<-2;theta[3,2,2]<-2;theta[4,3,2]<- -2;theta[5,4,2]<- -2;
theta[6,1,3]<-2;theta[8,2,3]<-2;theta[9,3,3]<- -2;theta[10,4,3]<- -2;
theta[6,1,4]<-2;theta[8,2,4]<-2;theta[9,3,4]<- -2;theta[10,4,4]<- -2;
theta[11,1,5]<-2;theta[13,2,5]<-2;theta[14,3,5]<- -2;theta[15,4,5]<- -2;
theta[11,1,6]<-2;theta[13,2,6]<-2;theta[14,3,6]<- -2;theta[15,4,6]<- -2

library(MASS)
pliable = matrix(0,N,6)
for (e in 1:6) {
pliable[,e]<-	compute_pliable(X, Z, theta[,,e])
}

esd<-diag(6)
e<-MASS::mvrnorm(N,mu=rep(0,6),Sigma=esd)
y_train<-X\%*\%Beta+pliable+e
y=y_train
#colnames(y)<-c(1:6)
colnames(y)<- c( paste("y",1:(ncol(y)),sep = "") )
TT=tree.parms(y)
plot(TT$h_clust)
gg1=matrix(0,2,2)
gg1[1,]<-c(0.02,0.02)
gg1[2,]<-c(0.02,0.02)

nlambda = 1
e.abs=1E-4
e.rel=1E-2
alpha=.2
tol=1E-3
fit <- MADMMplasso(
  X, Z, y, alpha=alpha, my_lambda=matrix(rep(0.2,dim(y)[2]),1),
  lambda_min=0.001, max_it=5000, e.abs=e.abs, e.rel=e.rel, maxgrid=nlambda,
  nlambda=nlambda, rho=5, tree=TT, my_print=FALSE, alph=1, parallel=FALSE,
  pal=1, gg=gg1, tol=tol, cl=6
)
}
